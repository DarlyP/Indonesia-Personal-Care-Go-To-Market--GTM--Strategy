{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "71caafc7",
      "metadata": {},
      "source": [
        "# **Indonesia Skincare Go-To-Market (GTM) Map Strategy**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36aa7147",
      "metadata": {},
      "source": [
        "This notebook processes Google Trends + Tokopedia + socio-economic indicators into a visualization-ready CSV for Looker Studio.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f98f9c9b",
      "metadata": {},
      "source": [
        "**Steps:**\n",
        "1. Place all CSV files in the `data/raw/` folder (see filename list below).\n",
        "2. Sort the cells from top to bottom.\n",
        "3. Fetch the results in `data/processed/` and connect to Looker/Looker Studio (via Google Sheets/BigQuery/Upload file).\n",
        "\n",
        "---\n",
        "\n",
        "**Expected CSV (in `data/raw/`):**\n",
        "- `geoMap_Province.csv` — column: `Province, Skincare_date`\n",
        "- `timeline.csv` — columns: `Time, Skincare_Search_Interst`\n",
        "- `Related_Topics.csv` — columns: `Related_Topics, Percentage`\n",
        "- `Tokopedia_data.csv` — columns: `product_id, price, discount_percentage, sold_count, rating, review_count, total_stock, category, brand_name_guess, province, price_band`\n",
        "- `Gini_ratio.csv` — columns: `Province, Gini Ratio ... 2024`\n",
        "- `Population_by_Density.csv` — columns: `Province, Population Density by Province (people/km2)`\n",
        "- `Population_by_Age.csv` — column: `Province, Number of Male/Female Population_... (age bins)`\n",
        "- `Average_monthly_per_capita.csv` — column: `Province, ... 2011..2024`\n",
        "- `Regional_Provincial_Wage.csv` — column: `Province, Regional/Provincial_Minimum_Wage_(Rupiah)_2020`\n",
        "\n",
        "> **Note**: The data obtained is still below 2025 but is considered consistent with the situation in 2025.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3680e3d3",
      "metadata": {},
      "source": [
        "## **Import Library**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cca35a79",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, pathlib\n",
        "import re, math\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dateutil import parser as dtp\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e371404f",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## **Data Handling**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f90f4019",
      "metadata": {},
      "source": [
        "### **Model structure**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAW_DIR : C:\\Users\\user\\Documents\\Coding\\Portofolio\\Indonesia_Skincare_GTM_Map\\data\\raw\n",
            "PROC_DIR: C:\\Users\\user\\Documents\\Coding\\Portofolio\\Indonesia_Skincare_GTM_Map\\data\\processed\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Setup folder structure\n",
        "BASE_DIR = pathlib.Path('.').resolve()\n",
        "RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
        "PROC_DIR = BASE_DIR / 'data' / 'processed'\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print('RAW_DIR :', RAW_DIR)\n",
        "print('PROC_DIR:', PROC_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e48be9",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### **Normalize Province**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize province names\n",
        "PROVINCE_MAP = {\n",
        "    # \"DKI JAKARTA\": \"DKI Jakarta\",\n",
        "    # \"DAERAH ISTIMEWA YOGYAKARTA\": \"DI Yogyakarta\",\n",
        "}\n",
        "\n",
        "def norm_province(x: str) -> str:\n",
        "    if pd.isna(x):\n",
        "        return x\n",
        "    s = str(x).strip()\n",
        "    s = PROVINCE_MAP.get(s.upper(), s.title())\n",
        "    return s\n",
        "\n",
        "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = [re.sub(r'\\s+', '_', c.strip()).lower() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def to_date(x):\n",
        "    if pd.isna(x):\n",
        "        return pd.NaT\n",
        "    try:\n",
        "        return dtp.parse(str(x)).date()\n",
        "    except Exception:\n",
        "        return pd.NaT\n",
        "\n",
        "def zscore(s):\n",
        "    s = pd.Series(s, dtype=float)\n",
        "    if s.std(ddof=0) == 0 or s.isna().all():\n",
        "        return pd.Series([0]*len(s), index=s.index)\n",
        "    return (s - s.mean())/s.std(ddof=0)\n",
        "\n",
        "def minmax(s):\n",
        "    s = pd.Series(s, dtype=float)\n",
        "    vmin, vmax = s.min(), s.max()\n",
        "    if pd.isna(vmin) or pd.isna(vmax) or vmin == vmax:\n",
        "        return pd.Series([0.5]*len(s), index=s.index)\n",
        "    return (s - vmin) / (vmax - vmin)\n",
        "\n",
        "def polyfit_slope(y, x=None):\n",
        "    if x is None:\n",
        "        x = np.arange(len(y))\n",
        "    try:\n",
        "        coeff = np.polyfit(x, y, 1)\n",
        "        return coeff[0]\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def read_csv_safe(path, **kwargs):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"[WARN] Missing file: {path}\")\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        df = pd.read_csv(path, **kwargs)\n",
        "    except Exception:\n",
        "        df = pd.read_csv(path, encoding=\"latin-1\", **kwargs)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37f22084",
      "metadata": {},
      "source": [
        "***Insight:***\n",
        "\n",
        "This script is an ETL/analytics toolkit that hardens the data pipeline for dependable business decisions: read_csv_safe robustly ingests CSVs (encoding fallback), clean_cols standardizes column names, norm_province harmonizes province keys for reliable joins, to_date parses dates, while minmax and zscore normalize metrics to comparable scales, and polyfit_slope summarizes trend momentum; together these utilities prevent broken joins, enable fair cross-province comparisons, and produce features needed to compute the opportunity score, price-to-nonfood, and sold per 100k ultimately driving province prioritization (Wave 1/Wave 2), pricing/assortment strategy (entry, mid-tier, premium-lite), stock/media allocation, and consistent ROI evaluation.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612e8f24",
      "metadata": {},
      "source": [
        "### **Geomap Handling**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processing: Geomap (interest per province)\n",
        "def process_geomap(path='data/raw/geoMap_Province.csv'):\n",
        "    df = read_csv_safe(path)\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = clean_cols(df).rename(columns={df.columns[0]: \"province\"})\n",
        "    df[\"province\"] = df[\"province\"].apply(norm_province)\n",
        "\n",
        "    # take all numeric columns → average as interest_province\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if not num_cols:\n",
        "        for c in df.columns[1:]:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    df[\"interest_province\"] = df[num_cols].mean(axis=1, skipna=True) if num_cols else np.nan\n",
        "\n",
        "    out = df[[\"province\", \"interest_province\"]]\n",
        "    out.to_csv('data/processed/trends_geomap_summary.csv', index=False)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d5190b2",
      "metadata": {},
      "source": [
        "***Insight:***\n",
        "\n",
        "process_geomap builds a province-level demand signal from the Google Trends Geomap file. It safely reads geoMap_Province.csv, cleans columns and normalizes the province key, coerces all date columns to numeric, then computes interest_province as the mean of all numeric columns per province. It outputs data/processed/trends_geomap_summary.csv with province, interest_province. Business purpose: provide a join-ready demand metric for the dashboard (“Search Interest by Province”), feed the demand component of the opportunity score, and highlight demand–supply gaps to drive activation priorities, stock/media allocation, and rollout sequencing.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c818aff",
      "metadata": {},
      "source": [
        "### **Related Topics Handling**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processing: Related Topics\n",
        "def process_related_topics(path='data/raw/Related_Topics.csv'):\n",
        "    df = read_csv_safe(path)\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = clean_cols(df).rename(columns={\"related_topics\": \"topic\"})\n",
        "    df[\"percentage_norm\"] = minmax(df[\"percentage\"])\n",
        "    df.to_csv('data/processed/related_topics_clean.csv', index=False)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8088f514",
      "metadata": {},
      "source": [
        "***Insight:***\n",
        "\n",
        "process_related_topics prepares Google Trends related topics for downstream use in dashboards and planning. It safely reads Related_Topics.csv, cleans columns and renames related_topics → topic for consistency, and computes a 0–1 weight percentage_norm = minmax(percentage), preserving the ranking while making the score comparable across slices (note: Trends percentage is a relative index, not share; normalization keeps ordering). The output is saved to data/processed/related_topics_clean.csv. Business purpose: deliver content and paid-media pillars (TOF education, MOF problem–solution, BOF product), enable a province × topic matrix to localize activation, and prioritize creative/keywords expected to lift CTR, conversion, and ultimately sold per 100k. Best-practice add-ons: dedupe & canonicalize topics (synonyms/bilingual), tag stage (TOF/MOF/BOF) and valid-count, and if province/time is present consider recency-weighted scoring or top-N per province.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e4e1bef",
      "metadata": {},
      "source": [
        "### **Tokopedia Handling**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processing: Tokopedia aggregated\n",
        "def process_tokopedia(path='data/raw/Tokopedia_data.csv'):\n",
        "    df = read_csv_safe(path)\n",
        "    if df.empty:\n",
        "        return df, pd.DataFrame()\n",
        "    df = clean_cols(df).rename(columns={\"brand_name_guess\": \"brand\"})\n",
        "    if \"province\" in df.columns:\n",
        "        df[\"province\"] = df[\"province\"].apply(norm_province)\n",
        "\n",
        "    agg = df.groupby(\"province\", dropna=False).agg(\n",
        "        median_price=(\"price\", \"median\"),\n",
        "        avg_price=(\"price\", \"mean\"),\n",
        "        sold=(\"sold_count\", \"sum\"),\n",
        "        avg_rating=(\"rating\", \"mean\"),\n",
        "        review=(\"review_count\", \"sum\"),\n",
        "        stock=(\"total_stock\", \"sum\"),\n",
        "        n_listing=(\"product_id\", \"count\")\n",
        "    ).reset_index()\n",
        "\n",
        "    # price_band distribution\n",
        "    if \"price_band\" in df.columns:\n",
        "        band = (df\n",
        "                .assign(count=1)\n",
        "                .pivot_table(index=\"province\", columns=\"price_band\", values=\"count\", aggfunc=\"sum\", fill_value=0))\n",
        "        band.columns = [f\"band_{str(c)}\" for c in band.columns]\n",
        "        agg = agg.merge(band.reset_index(), on=\"province\", how=\"left\")\n",
        "\n",
        "    agg.to_csv('data/processed/tokopedia_agg_by_province.csv', index=False)\n",
        "\n",
        "    weekly = agg.copy()\n",
        "    weekly[\"week\"] = pd.Timestamp.today().date().isoformat()\n",
        "    weekly = weekly[[\"province\", \"week\", \"sold\"]]\n",
        "\n",
        "    return agg, weekly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f47664",
      "metadata": {},
      "source": [
        "***Insight:***\n",
        "\n",
        "provided process_tokopedia as a core ETL block that converts raw Tokopedia listings into province-level supply & pricing signals for dashboards and scoring. It: ingests and cleans columns (renames brand_name_guess→brand, normalizes province), aggregates by province to compute median_price, avg_price, total sold (units), avg_rating, total review, total stock, and n_listing, builds a price-band distribution via pivot (creating band_* columns for counts per price tier), and exports to data/processed/tokopedia_agg_by_province.csv; it also emits a simple weekly snapshot (province, week, sold) for trend tracking. Business purpose: this feeds the Marketplace Summary, Median Price by Province, and Price Band Distribution views, and supplies inputs for key metrics like sold_per_100k = sold/population_total×100000 (penetration) and price_to_nonfood = median_price/avg_monthly_nonfood_2024 (affordability), which roll into the opportunity score and drive tactical choices (entry/value vs premium-lite, stock/media allocation, rollout sequencing).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b27b437a",
      "metadata": {},
      "source": [
        "### **Gini Handling**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processing: Gini \n",
        "def process_gini(path='data/raw/Gini_ratio.csv'):\n",
        "    df = read_csv_safe(path)\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = clean_cols(df)\n",
        "    if \"province\" not in df.columns:\n",
        "        df.rename(columns={df.columns[0]: \"province\"}, inplace=True)\n",
        "    df[\"province\"] = df[\"province\"].apply(norm_province)\n",
        "\n",
        "    target_cols = [c for c in df.columns if \"urban_rural\" in c and (\"semester_2\" in c or \"september_2024\" in c)]\n",
        "    if not target_cols:\n",
        "        target_cols = [c for c in df.columns if \"2024\" in c.lower()]\n",
        "    latest = df[target_cols].mean(axis=1, skipna=True) if target_cols else np.nan\n",
        "    out = df[[\"province\"]].copy()\n",
        "    out[\"gini_2024\"] = latest\n",
        "    out.to_csv('data/processed/gini_latest.csv', index=False)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c98d3e81",
      "metadata": {},
      "source": [
        "***Insight:***\n",
        "\n",
        "process_gini turns the Gini ratio into a join-ready provincial feature. It safely reads the CSV, cleans columns and normalizes province, selects the latest columns—preferring those containing urban_rural and semester_2/september_2024 (proxy for H2/Sept 2024 release); if missing, it falls back to any 2024 columns and computes an average as gini_2024, and (4) writes province, gini_2024 to data/processed/gini_latest.csv. Business purpose: provide a slow-moving inequality signal that informs assortment & pricing: high Gini ⇒ barbell assortment (value + premium simultaneously) and hyper-local targeting rather than province-wide averages. It can also act as a penalty/moderator in the Opportunity Score, improving rollout prioritization, media/stock allocation, and premium-lite vs value decisions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c53ee5",
      "metadata": {},
      "source": [
        "### **Population Density Handling**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processing: Population density\n",
        "def process_density(path='data/raw/Population_by_Density.csv'):\n",
        "    df = read_csv_safe(path)\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = clean_cols(df)\n",
        "    if \"province\" not in df.columns:\n",
        "        df.rename(columns={df.columns[0]: \"province\"}, inplace=True)\n",
        "    df[\"province\"] = df[\"province\"].apply(norm_province)\n",
        "\n",
        "    for c in df.columns:\n",
        "        if \"density\" in c:\n",
        "            den_col = c\n",
        "            break\n",
        "    else:\n",
        "        den_col = df.columns[1]\n",
        "    out = df[[\"province\", den_col]].rename(columns={den_col: \"density_2021\"})\n",
        "    out.to_csv('data/processed/density_2021.csv', index=False)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d29cee",
      "metadata": {},
      "source": [
        "***Insight:***\n",
        "\n",
        "process_density prepares the province-level population density as an “urban reach” signal for analytics and scoring. It: safely reads Population_by_Density.csv, cleans columns and normalizes the province key, locates the column containing “density” (or falls back to the second column), and outputs only province and density_2021 to data/processed/density_2021.csv. Business purpose: density proxies distribution and media reach efficiency (more audience per touchpoint, cheaper last-mile) and indicates brand/premium readiness—urban markets are more competitive but high-value. This feeds the province_master table, the Population Density visual, and the density_norm component in the opportunity score (e.g., weight +0.1), which informs rollout prioritization, stock/media allocation, and pricing/assortment strategy by province.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c815b321",
      "metadata": {},
      "source": [
        "### **Population by Age Handling**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processing: Population by Age\n",
        "def process_pop_age(path='data/raw/Population_by_Age.csv'):\n",
        "    df = read_csv_safe(path)\n",
        "    if df.empty:\n",
        "        return df, pd.DataFrame()\n",
        "    df = clean_cols(df)\n",
        "    if \"province\" not in df.columns:\n",
        "        df.rename(columns={df.columns[0]: \"province\"}, inplace=True)\n",
        "    df[\"province\"] = df[\"province\"].apply(norm_province)\n",
        "\n",
        "    female_cols = [c for c in df.columns if c.startswith(\"number_of_female_population_\")]\n",
        "    male_cols = [c for c in df.columns if c.startswith(\"number_of_male_population_\")]\n",
        "    df[\"female_total\"] = df[female_cols].sum(axis=1, skipna=True) if female_cols else np.nan\n",
        "    df[\"male_total\"] = df[male_cols].sum(axis=1, skipna=True) if male_cols else np.nan\n",
        "    df[\"population_total\"] = df[\"female_total\"] + df[\"male_total\"]\n",
        "\n",
        "    def sum_age(prefix, bins):\n",
        "        cols = [f\"{prefix}{b}\" for b in bins]\n",
        "        cols = [c for c in cols if c in df.columns]\n",
        "        return df[cols].sum(axis=1, skipna=True) if cols else np.nan\n",
        "\n",
        "    bins_15_44 = [\"15-19\",\"20-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\"]\n",
        "    bins_20_39 = [\"20-24\",\"25-29\",\"30-34\",\"35-39\"]\n",
        "\n",
        "    df[\"female_15_44\"] = sum_age(\"number_of_female_population_\", bins_15_44)\n",
        "    df[\"female_20_39\"] = sum_age(\"number_of_female_population_\", bins_20_39)\n",
        "\n",
        "    out = df[[\"province\",\"population_total\",\"female_total\",\"male_total\",\"female_15_44\",\"female_20_39\"]]\n",
        "    out.to_csv('data/processed/population_age_summary.csv', index=False)\n",
        "    return out, df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14707fd",
      "metadata": {},
      "source": [
        "***Insight:***\n",
        "\n",
        "provided process_pop_age to turn raw demographic tables into an actionable TAM at the provincial level. It safely reads the CSV, standardizes columns and normalizes the province key for reliable joins, sums all female/male age columns into female_total and male_total and builds population_total, and robustly aggregates target age bands (only summing existing columns) to produce female_15_44 and female_20_39. The concise output is written to data/processed/population_age_summary.csv with province, population_total, female_total, male_total, female_15_44, female_20_39. Business purpose: provide denominators and targetable TAM for core metrics:\n",
        "\n",
        "- Enable per-capita comparisons (e.g., sold_per_100k = sold / population_total × 100000)\n",
        "- Construct the Demand Proxy (interest_province × female_20_39 / population_total) to prioritize provinces \n",
        "- Guide stock/media allocation by core segment size, and \n",
        "- Feed TAM visuals (absolute & per-100k) and bubble sizes in scatter charts.\n",
        "\n",
        "Best-practice adds: compute female_20_39_per_100k and female_share_20_39 inside this function, enforce clean numeric types, align the demographic year with other datasets, and log provinces with incomplete age bands."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "672249a9",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### **Average monthly per capita Handling**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processing: Average monthly per capita \n",
        "def process_expenditure(path='data/raw/Average_monthly_per_capita.csv'):\n",
        "    df = read_csv_safe(path)\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = clean_cols(df)\n",
        "    if \"province\" not in df.columns:\n",
        "        df.rename(columns={df.columns[0]: \"province\"}, inplace=True)\n",
        "    df[\"province\"] = df[\"province\"].apply(norm_province)\n",
        "\n",
        "    cands = [c for c in df.columns if c.endswith(\"_2024\") or c.endswith(\"2024\")]\n",
        "    if not cands:\n",
        "        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        cands = [num_cols[-1]] if num_cols else []\n",
        "    col_2024 = cands[0] if cands else df.columns[-1]\n",
        "    out = df[[\"province\", col_2024]].rename(columns={col_2024: \"avg_monthly_nonfood_2024\"})\n",
        "    out.to_csv('data/processed/avg_monthly_nonfood_2024.csv', index=False)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a34a8a7",
      "metadata": {},
      "source": [
        "***Insight:***\n",
        "\n",
        "provided process_expenditure to build a purchasing-power indicator—the average monthly non-food expenditure per capita (2024) at the provincial level—which serves as the denominator for affordability. The function safely reads the CSV, standardizes columns and normalizes the province key for reliable joins, detects the 2024 column (suffix _2024/2024; if missing, falls back to the last numeric column), and outputs province, avg_monthly_nonfood_2024 to data/processed/avg_monthly_nonfood_2024.csv. Business purpose: this feeds price_to_nonfood = median_price / avg_monthly_nonfood_2024, which rolls into the Opportunity Score, guides entry vs premium-lite pricing, stock/media allocation, and rollout sequencing (Wave 1 for affordable markets; Wave 2 for value-uplift).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99ff6d78",
      "metadata": {},
      "source": [
        "### **Regional/Provincial Wage Handling**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processing: Regional/Provincial Wage\n",
        "def process_wage(path='data/raw/Regional_Provincial_Wage.csv'):\n",
        "    df = read_csv_safe(path)\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df = clean_cols(df)\n",
        "    if \"province\" not in df.columns:\n",
        "        df.rename(columns={df.columns[0]: \"province\"}, inplace=True)\n",
        "    df[\"province\"] = df[\"province\"].apply(norm_province)\n",
        "\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    col = num_cols[0] if num_cols else df.columns[1]\n",
        "    out = df[[\"province\", col]].rename(columns={col: \"ump_2020\"})\n",
        "    out.to_csv('data/processed/ump_2020.csv', index=False)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21777e97",
      "metadata": {},
      "source": [
        "***Insight:***\n",
        "\n",
        "process_wage produces a macro purchasing-power indicator—the provincial minimum wage (UMP)—as a join-ready feature. It safely reads the CSV, cleans columns and normalizes the province key, picks the first numeric column as the wage value and labels it ump_2020, and exports province, ump_2020 to data/processed/ump_2020.csv. Business purpose: UMP serves as a baseline purchasing-power screen to guide entry vs step-up/premium-lite pricing, to validate price_to_nonfood insights, and to optionally moderate the Opportunity Score. Practically, it helps distinguish premium-ready markets from those needing trial/value offers.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd73d30",
      "metadata": {},
      "source": [
        "### **Province Master, Demand vs Supply, Opportunity Score, Weekly Join Handling**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Builders: province_master, demand_vs_supply, opportunity_score, weekly join\n",
        "def build_province_master(gini, density, pop_sum, expend, wage):\n",
        "    out = None\n",
        "    for f in [gini, density, pop_sum, expend, wage]:\n",
        "        if f is None or f.empty:\n",
        "            continue\n",
        "        out = f if out is None else out.merge(f, on=\"province\", how=\"outer\")\n",
        "    if out is not None and \"population_total\" in out.columns:\n",
        "        out[\"female_share\"] = out[\"female_total\"] / out[\"population_total\"]\n",
        "    if out is not None:\n",
        "        out.to_csv('data/processed/province_master.csv', index=False)\n",
        "    return out\n",
        "\n",
        "def demand_vs_supply(trends_geomap, tokopedia_agg, pop_summary):\n",
        "    if trends_geomap is None or trends_geomap.empty or tokopedia_agg is None or tokopedia_agg.empty:\n",
        "        return pd.DataFrame()\n",
        "    df = trends_geomap.merge(tokopedia_agg, on=\"province\", how=\"outer\")\n",
        "    if pop_summary is not None and not pop_summary.empty:\n",
        "        df = df.merge(pop_summary[[\"province\",\"population_total\",\"female_20_39\"]], on=\"province\", how=\"left\")\n",
        "    df[\"female_20_39_share\"] = df[\"female_20_39\"] / df[\"population_total\"]\n",
        "    df[\"demand_proxy\"] = df[\"interest_province\"] * df[\"female_20_39_share\"]\n",
        "    df[\"sold_per_100k\"] = (df[\"sold\"] / df[\"population_total\"]) * 1e5\n",
        "    return df\n",
        "\n",
        "def opportunity_score(demand_supply_df, province_master):\n",
        "    if demand_supply_df is None or demand_supply_df.empty:\n",
        "        return pd.DataFrame()\n",
        "    df = demand_supply_df.copy()\n",
        "    if province_master is not None and not province_master.empty:\n",
        "        df = df.merge(province_master[[\"province\",\"avg_monthly_nonfood_2024\",\"ump_2020\",\"gini_2024\",\"density_2021\"]], on=\"province\", how=\"left\")\n",
        "    df[\"price_to_nonfood\"] = df[\"median_price\"] / df[\"avg_monthly_nonfood_2024\"]\n",
        "\n",
        "    demand_norm = minmax(df[\"demand_proxy\"])\n",
        "    supply_norm = minmax(df[\"sold_per_100k\"])\n",
        "    afford_pen = minmax(df[\"price_to_nonfood\"])\n",
        "    gini_pen = minmax(df[\"gini_2024\"])\n",
        "    density_norm = minmax(df[\"density_2021\"])\n",
        "\n",
        "    score = (0.5*demand_norm - 0.3*supply_norm - 0.1*afford_pen + 0.1*density_norm - 0.0*gini_pen)\n",
        "    score = minmax(score) * 100.0\n",
        "    df[\"opportunity_score\"] = score.round(2)\n",
        "\n",
        "    cols = [\"province\",\"interest_province\",\"median_price\",\"sold\",\"sold_per_100k\",\n",
        "            \"female_20_39\",\"demand_proxy\",\"avg_monthly_nonfood_2024\",\"ump_2020\",\n",
        "            \"gini_2024\",\"density_2021\",\"price_to_nonfood\",\"opportunity_score\"]\n",
        "    cols = [c for c in cols if c in df.columns]\n",
        "    out = df[cols].sort_values(\"opportunity_score\", ascending=False)\n",
        "    out.to_csv('data/processed/opportunity_score_by_province.csv', index=False)\n",
        "    return out\n",
        "\n",
        "def build_weekly_join(timeline_weekly, tokopedia_weekly):\n",
        "    if timeline_weekly is None or timeline_weekly.empty or tokopedia_weekly is None or tokopedia_weekly.empty:\n",
        "        return pd.DataFrame()\n",
        "    df = tokopedia_weekly.merge(timeline_weekly[[\"week\",\"interest\"]], on=\"week\", how=\"left\")\n",
        "    df.to_csv('data/processed/demand_vs_supply_weekly.csv', index=False)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c82300be",
      "metadata": {},
      "source": [
        "***Insight:***\n",
        "\n",
        "This “Builders” block is the backbone that fuses all features into decision-ready datasets for the dashboard and ROI tracking:\n",
        "\n",
        "1. build_province_master(...): Outer-joins Gini, density, demographics, 2024 non-food spend, and UMP into a single province master; derives female_share. Output: province_master.csv.\n",
        "Business link: one-stop, consistent lookup for affordability/TAM/reach/inequality—foundation for pricing, assortment, segmentation, and rollout.\n",
        "\n",
        "2. demand_vs_supply(...): merges demand (interest_province) with marketplace supply and demographics; derives female_20_39_share, demand_proxy = interest * share, and sold_per_100k.\n",
        "Business link: core table to spot demand–supply gaps vs mature markets—drives activation vs value-uplift plays.\n",
        "\n",
        "3. opportunity_score(...): adds macros, computes price_to_nonfood, min–max normalizes features, then scores:\n",
        "0.5*demand_norm - 0.3*supply_norm - 0.1*afford_pen + 0.1*density_norm - 0.0*gini_pen,\n",
        "rescales to 0–100, outputs opportunity_score_by_province.csv.\n",
        "Business link: a prioritization compass balancing potential (demand, density) vs constraints (high supply, affordability), guiding rollout sequencing, stock/media allocation, and entry/value vs premium-lite decisions.\n",
        "\n",
        "4. build_weekly_join(...): joins weekly interest with weekly sold by week; outputs demand_vs_supply_weekly.csv.\n",
        "Business link: track co-movement of interest and sales, measure post-activation Δ sold per 100k, and support evidence-based scaling.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7f5130f",
      "metadata": {},
      "source": [
        "### **Pipeline Running**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DONE ===\n",
            "Output CSV:\n",
            " - data/processed\\avg_monthly_nonfood_2024.csv\n",
            " - data/processed\\density_2021.csv\n",
            " - data/processed\\gini_latest.csv\n",
            " - data/processed\\opportunity_score_by_province.csv\n",
            " - data/processed\\population_age_summary.csv\n",
            " - data/processed\\province_master.csv\n",
            " - data/processed\\related_topics_clean.csv\n",
            " - data/processed\\tokopedia_agg_by_province.csv\n",
            " - data/processed\\trends_geomap_summary.csv\n",
            " - data/processed\\trends_timeline_enhanced.csv\n",
            " - data/processed\\ump_2020.csv\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Run Pipeline\n",
        "timeline_df, timeline_weekly = process_timeline()\n",
        "geomap_df = process_geomap()\n",
        "related_topics_df = process_related_topics()\n",
        "tokopedia_agg, toko_weekly = process_tokopedia()\n",
        "\n",
        "gini = process_gini()\n",
        "density = process_density()\n",
        "pop_sum, pop_full = process_pop_age()\n",
        "expend = process_expenditure()\n",
        "wage = process_wage()\n",
        "\n",
        "prov_master = build_province_master(gini, density, pop_sum, expend, wage)\n",
        "dsv = demand_vs_supply(geomap_df, tokopedia_agg, pop_sum)\n",
        "opp = opportunity_score(dsv, prov_master)\n",
        "weekly_join = build_weekly_join(timeline_weekly, toko_weekly)\n",
        "\n",
        "# Print summary of output files\n",
        "print(\"=== DONE ===\")\n",
        "print(\"Output CSV:\")\n",
        "for p in sorted(glob('data/processed/*.csv')):\n",
        "    print(\" -\", p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87833a3d",
      "metadata": {},
      "source": [
        "***Insight:***\n",
        "\n",
        "Data that has been successfully generated can be used for visualization.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
